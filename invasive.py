# -*- coding: utf-8 -*-
"""Invasive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JE6nTnEbKD7CD2QBdkl6HBlaX6hdyRCy
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt #show images
import os #navigate directories & merge paths
import cv2 #access and manipualate images

from google.colab import drive 
drive.mount('/content/drive')

#sets the drive extraction up

DATADIR = "/content/drive/My Drive/Invasive_Dataset"
CATEGORIES = ["Autumn_Olive", "Canada_Thistle", "Common_Tansy", "Dog_Strangling_Vine", "Emerald_Ash_Borer", "Eurasian_Milfoil", "European_Buckthorn", "European_Green_Crab", "Non_Invasive"]
print("Finished")

training_data = [] 

IMG_SIZE = 180
#for resizing images

def create_training_data():
    for category in CATEGORIES:
      path = os.path.join(DATADIR, category)
      class_num = CATEGORIES.index(category)
      print("on {}".format(class_num))
      for img in os.listdir(path):
        print("getting image")
        try:
          img_array = cv2.imread(os.path.join(path, img))
          resized_image = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))
          training_data.append([resized_image, class_num])
        except Exception as e:
          pass


create_training_data()

print(len(training_data))



#NOTE: 0 - Autumn_Olive, 1- Canada_Thistle, 2 - Common_Tansy, 3 - Dog_Strangling_Vine, 4 - Emerald_Ash_Borer, 5 - Eurasian_Milfoil, 6 - European_Buckthorn, 7 - European_Green_Crab and possibly  8 - Non_Invasive

import random
random.shuffle(training_data)

#this shuffles the array so that the algorithm doesn't simply learn the order and guess based on that

X = []
y = []

for image, label in training_data:
  X.append(image)
  y.append(label)

plt.imshow(X[0])
plt.show()


X = np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3) #reshapes the image array, the -1 indicates an indifference to the input size, the dimensions and finally the "1" indicates gray scale
#code above adds the different parameters to the x and y lists, so the NN can try to match the image with its label

#starting to create the CNN 
#convolution, implies taking a window over the image, and simplifying the window to some sort of value
#pooling, implies pooling the values in a window together and finding returning the max value
 
#in general, pooling implies max pooling
 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
#we import these so that we dont have to call the .keras.methods or .keras.layers methods to access them
 
 
X = tf.keras.utils.normalize(X,axis = 1) #normalizes the values to be around 1
 
model = Sequential() #usually choose some nonlinear model 
 
#notice, how convolude and pool are 2D functions
model.add( Conv2D(128, (5,5), input_shape = (IMG_SIZE,IMG_SIZE,3)) )
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(5,5)))
 
model.add(Dropout(0.2))
model.add( Conv2D(64, (5,5)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(3,3)))
 
model.add( Conv2D(32, (2,2)))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

 
#here we must flatten the arrays, so that they can be used with the 1D Dense function
model.add(Flatten())
model.add(Dense(32))
 
model.add(Dense(9))#9 nodes for 9 outputs
model.add(Activation("softmax"))
 
model.compile(loss = "sparse_categorical_crossentropy",
              optimizer = "adam",
              metrics = ['accuracy'])
 
model.fit(X, y , validation_split = 0.2, epochs = 30 )

#model.save('Invasive Species Detector')

predictions = model.predict(X[0])

print(np.argmax(predictions[14]))

print(CATEGORIES[np.argmax(predictions[14])])

plt.imshow(X[14])
plt.show()